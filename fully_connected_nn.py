# -*- coding: utf-8 -*-
"""fully_connected_nn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15BYFOB7oysnw9dt1gPU7c2O8Rl5UOGtE
"""

import numpy as np
import matplotlib.pyplot as plt

class FullyConnectedNN(object):

    def __init__(self, input_size, hidden_size, output_size, lr=1e-2, varying_lr=False, varying_lr_loss_horizon=4, varying_lr_threshold=1e-3, varying_lr_coefficient=0.5):
        """
        Δέχεται τις παραμέτρους input_size, hidden_size, output_size. To hidden_size το δέχεται στη μορφή [m, n], όπου
        m είναι το πλήθος των hidden layers και n ο αριθμός των νευρώνων ανά hidden layer. Επιτρέπονται λοιπόν πολλά
        hidden layers, αν και υπό τον περιορισμό ότι όλα έχουν το ίδιο μέγεθος.
        Όσον αφορά το learning rate, έχω προσθέσει, πέραν του lr, και τις παραμέτρους varying_lr, varying_lr_loss_horizon,
        varying_lr_threshold, varying_lr_coefficient.
        Αυτά χρησιμοποιούνται από το step function προκειμένου να προβεί σε μια μορφή απλού scheduling του learning rate,
        όταν varying_lr=True. Αποθηκεύονται οι μεταβολές στο loss ανά iteration στη λίστα model_diff_losses του init,
        υπολογίζεται ο μέσος όρος για τις varying_lr_loss_horizon (στο πλήθος) τελευταίες, και αν αυτός προκύπτει μικρότερος του
        varying_lr_threshold τότε πολλαπλασιάζεται το lr με το varying_lr_coefficient.
        """
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_hidden_layers = hidden_size[0]         # το hidden layer δίνεται υπό τη μορφή [m, n], όπου m ο αριθμός των hidden layers
        self.neurons_per_hidden_layer = hidden_size[1]  # από το [m, n], το n αντιπροσωπεύει τον αριθμό των νευρώνων ανά hidden layer
        self.output_size = output_size
        self.lr = lr
        self.varying_lr = varying_lr
        self.varying_lr_loss_horizon = varying_lr_loss_horizon
        self.varying_lr_threshold = varying_lr_threshold
        self.varying_lr_coefficient = varying_lr_coefficient
        self.model_weights = []       # λίστα στην οποία αποθηκεύουμε τους πίνακες με τα βάρη
        self.model_biases = []        # λίστα για τα biases
        self.grad_weights = []        # λίστα αποθήκευσης των weight gradients, προκειμένου να υποβοηθούν οι υπολογισμοί κατά το backward pass
        self.grad_biases = []         # λίστα αποθήκευσης των bias gradients
        self.layer_linear_outputs=[]  # λίστα αποθήκευσης του linear output (προ της τροφοδότησης στο αντίστοιχο activation function) για το κάθε layer
        self.layer_outputs = []       # λίστα αποθήκευσης του activation output για το κάθε layer
        self.model_losses = []        # λίστα αποθήκευσης των losses
        self.model_diff_losses = []   # λίστα αποθήκευσης των μεταβολών του loss από το εκάστοτε iteration στο επόμενο
        self.deltas = []              # λίστα αποθήκευσης του delta για το επόμενο layer επί τον transposed πίνακα βαρών για το τρέχον (κατά το backward pass), επί της παραγώγου του activation ως προς το linear section
        self.num_train_samples = None
        self.init_parameters()

    def init_parameters(self, mu=0.0, sigma=0.1):
        """
        Συνάρτηση που κάνει initialize τα weights και biases του μοντέλου, καθώς και των gradients γι αυτά.
        Initialization βάσει normal distribution για weights και biases. Gradients απλά μηδενίζονται.
        """
        self.model_weights = []
        self.model_biases = []
        self.grad_weights = []
        self.grad_biases = []

        # initialization παραμέτρων από πρώτο layer (input) προς πρώτο hidden
        W = np.random.normal(mu, sigma, size=(self.neurons_per_hidden_layer, self.input_size)) # https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html#numpy.random.normal
        b = np.random.normal(mu, sigma, size=(self.neurons_per_hidden_layer, 1))
        self.model_weights.append(W)
        self.model_biases.append(b)
        self.grad_weights.append(np.zeros_like(W))
        self.grad_biases.append(np.zeros_like(b))

        # initialization των hidden παραμέτρων
        for _ in range(self.num_hidden_layers - 1):
            W = np.random.normal(mu, sigma, size=(self.neurons_per_hidden_layer, self.neurons_per_hidden_layer))
            b = np.random.normal(mu, sigma, size=(self.neurons_per_hidden_layer, 1))
            self.model_weights.append(W)
            self.model_biases.append(b)
            self.grad_weights.append(np.zeros_like(W))
            self.grad_biases.append(np.zeros_like(b))

        # initialization των παραμέτρων από το τελευταίο hidden layer προς το output layer
        W = np.random.normal(mu, sigma, size=(self.output_size, self.neurons_per_hidden_layer))
        b = np.random.normal(mu, sigma, size=(self.output_size, 1))
        self.model_weights.append(W)
        self.model_biases.append(b)
        self.grad_weights.append(np.zeros_like(W))
        self.grad_biases.append(np.zeros_like(b))

    def forward(self, x):
        """
        Συνάρτηση για forward pass. Linear portion του κάθε layer υπολογίζεται ως Weights x Layer_input + Biases
        Hidden Layers χρησιμοποιούν ReLU ως activation function, output layer sigmoid (για έναν νευρώνα) ή softmax.
        """
        activation = x
        self.layer_outputs = [x]
        self.layer_linear_outputs = []

        for W, b in zip(self.model_weights[:-1], self.model_biases[:-1]):
            z = np.dot(W, activation) + b
            self.layer_linear_outputs.append(z)
            activation = self.relu(z)
            self.layer_outputs.append(activation)

        W_out = self.model_weights[-1]
        b_out = self.model_biases[-1]
        z_out = np.dot(W_out, activation) + b_out
        self.layer_linear_outputs.append(z_out)
        if self.output_size == 1:
            output = self.sigmoid(z_out)
        else:
            output = self.softmax(z_out)
        self.layer_outputs.append(output)
        return output


    def backward(self, y_true):
      """
      Συνάρτηση για backward pass. Στηρίζεται σε μεγάλο βαθμό στις λίστες αποθήκευσης για linear και activation outputs της κλάσης init,
      καθώς και στην λίστα deltas, όπου αποθηκεύονται ενδιάμεσα στάδια του bacward pass. Weight και bias gradients αποθηκεύονται στις
      αντίστοιχες λίστες της init, προκειμένου να διευκολυνθεί εν συνεχεία η συνάρτηση step.
      """
      self.deltas = []
      L = self.num_hidden_layers

      a_out = self.layer_outputs[-1]
      delta = a_out - y_true  # παράγωγος του cross entropy ως προς sigmoid/softmax output activation επί παράγωγο του τελευταίου ως προς linear output απλοποιείται σε αυτό
      self.deltas.insert(0, delta)

      for i in reversed(range(L)):
        z = self.layer_linear_outputs[i]
        relu_deriv = np.greater(z, 0).astype(float) # https://stackoverflow.com/questions/46411180/implement-relu-derivative-in-python-numpy (σημείωση: αυτό θα μπορούσε εναλλακτικά να είχε οριστεί ως ξεχωριστό static method)
        W_next = self.model_weights[i + 1]
        delta_next = self.deltas[0]
        delta = np.dot(W_next.T, delta_next) * relu_deriv
        self.deltas.insert(0, delta)

      # υπολογισμός και αποθήκευση των weight και bias gradients:
      for i in range(L + 1):
        dZ = self.deltas[i]
        A_prev = self.layer_outputs[i]
        self.grad_weights[i][:] = np.dot(dZ, A_prev.T)
        self.grad_biases[i][:] = np.sum(dZ, axis=1, keepdims=True)


    def step(self, m=None):
        """
        Συνάρτηση βήματος που εκτελεί mini-batch gradient descent, βάσει των batches που της παρέχει η fit function.
        Χρησιμοποιούνται οι πίνακες που έχουν αποθηκευτεί εντός των λιστών grad_weights, grad_biases της init, προκειμένου
        να ανανεωθούν επί τόπου οι τιμές που έχουν αποθηκευτεί στους πίνακες των model_weights, model_biases της init.
        """
        if m is None:
            raise ValueError("Πρέπει να δωθεί το μέγεθος m των batches.")

        for i in range(len(self.model_weights)):
            self.model_weights[i] -= self.lr * (self.grad_weights[i] / m)
            self.model_biases[i] -= self.lr * (self.grad_biases[i] / m)

    def predict(self, x, threshold=0.5):
        """
        Μια συνάρτηση predict. Επί της ουσίας απλά κάνει ένα forward pass, και εφαρμόζει ένα threshold αν το τελευταίο activation
        είναι sigmoid (binary case), argmax αν το τελευταίο activation είναι softmax (multiclass case).
        """
        activation = x
        for W, b in zip(self.model_weights[:-1], self.model_biases[:-1]):
            z = np.dot(W, activation) + b
            activation = self.relu(z)

        W_out = self.model_weights[-1]
        b_out = self.model_biases[-1]
        z_out = np.dot(W_out, activation) + b_out

        if self.output_size == 1:
            output = self.sigmoid(z_out)
            return (output > threshold).astype(int)
        else:
            output = self.softmax(z_out)
            return np.argmax(output, axis=0)


    def fit(self, X, y, iterations=10000, batch_size=None, show_step=1000):
        """
        Συνάρτηση fit. Κάνει raise ValueError αν το input δεν έχει δωθεί υπό την μορφή numpy arrays, ή αν δεν είναι του σωστού μήκους.
        Χωρίζει feature και label sets σε batches, και επανειλημμένα εφαρμόζει διαδοχικά τις συναρτήσεις forward, backward, και step.
        Loss υπολογίζεται ανά show_step στο πλήθος βήματα και αποθηκεύεται στη λίστα model_losses του init. Αν το varying_lr είναι
        True τότε μεταβάλλει και το lr αναλόγως του αν πληρούνται οι σχετικές συνθήκες. Τέλος, κάνει call και συνάρτηση show_all_model_heatmaps_combined
        προκειμένου να τυπώσει visualization των weights του κάθε layer σε heatmaps, κατά τη διάρκεια της εκπαίδευσης.
        """
        if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):
            raise ValueError("X, y πρέπει να δωθούν ως numpy arrays.")
        if X.shape[0] != y.shape[0]:
            raise ValueError("Μήκος Χ ορίσματος διαφέρει από μήκος του y.")

        X = X.T
        y = y.T
        N = X.shape[1]
        self.num_train_samples = N

        indices = np.arange(N)

        for i in range(1, iterations + 1):
            if i == 1 or (batch_size and (i - 1) * batch_size % N == 0):
                np.random.shuffle(indices)

            if batch_size is None:
                np.random.shuffle(indices)
                batch_indices = indices
            else:
                start = ((i - 1) * batch_size) % N
                end = start + batch_size
                if end > N:
                    batch_indices = np.concatenate([indices[start:], indices[:end - N]])
                else:
                    batch_indices = indices[start:end]

            X_batch = X[:, batch_indices]
            y_batch = y[:, batch_indices]

            y_pred = self.forward(X_batch)
            self.backward(y_batch)
            self.step(m=y_batch.shape[1])

            if show_step and i % show_step == 0:
                y_pred_all = self.forward(X)
                loss = self.cross_entropy_loss(y_pred_all, y)
                self.model_losses.append(loss)
                print("================================")
                print(f"Iteration {i}, Loss: {loss:.4f}")
                print("================================")
                self.show_all_model_heatmaps_combined()
                if self.varying_lr and len(self.model_losses)>1:
                  self.model_diff_losses.append(self.model_losses[-2]-self.model_losses[-1])
                  if len(self.model_diff_losses) >= self.varying_lr_loss_horizon:
                    if np.mean(self.model_diff_losses[-self.varying_lr_loss_horizon:])<=self.varying_lr_threshold:
                      print("------------------------------------------")
                      print("|Learning Rate changed for next Iteration!|")
                      print("------------------------------------------")
                      self.lr*=self.varying_lr_coefficient

    # παρακάτω ορίζονται οι συναρτήσεις sigmoid, relu, softmax, και το cross entropy loss, ως static methods:

    @staticmethod
    def sigmoid(z):
        return 1.0 / (1.0 + np.exp(-z))

    @staticmethod
    def relu(z):
        return np.maximum(0, z)

    @staticmethod
    def softmax(z):
        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))
        return exp_z / np.sum(exp_z, axis=0, keepdims=True)

    @staticmethod
    def cross_entropy_loss(y_pred, y_true, epsilon=1e-12): # έχω προσαρμόσει την πρώτη απάντηση από: https://stackoverflow.com/questions/47377222/what-is-the-problem-with-my-implementation-of-the-cross-entropy-function
        y_pred = np.clip(y_pred, epsilon, 1. - epsilon) # κάνουμε clipping με epsilon>0 για να αποφύγουμε log(0), log(1)
        loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[1] # σημείωση: η fit έχει προηγουμένως κάνει transpose το y_true, λόγω του τρόπου με τον οποίο έχει σχεδιαστεί η τρέχουσα υλοποίηση
        return loss

    # ------------------------------------------------------
    # παρακάτω ορίζονται συναρτήσεις που χρησιμοποιούνται για οπτικοποίηση των weights και biases ως heatmaps:


    def visualize_layer_heatmap(self, layer, level='Weights', cmap="bwr", ax=None):
      if level == 'Weights':
        matrix = self.model_weights[layer]
        title = f"Weights for layer {layer+1}"
      else:
        matrix = self.model_biases[layer]
        title = f"Biases for layer {layer+1}"

      standalone = ax is None
      if standalone:
        fig, ax = plt.subplots(figsize=(6, 4))

      im = ax.imshow(matrix, cmap=cmap, aspect='auto')
      ax.set_title(title)

      ax.set_xticks(np.arange(matrix.shape[1]))
      ax.set_yticks(np.arange(matrix.shape[0]))
      ax.set_xticklabels(np.arange(matrix.shape[1]))
      ax.set_yticklabels(np.arange(matrix.shape[0]))

      ax.grid(False)

      if standalone:
        plt.colorbar(im, ax=ax)
        plt.tight_layout()
        plt.show()


    def show_all_model_heatmaps_combined(self, show_biases=False):
      """
      Συνδυάζει τα heatmaps σε ενιαίο πλαίσιο. Αν show_biases είναι True τότε θα εμφανιστούν και τα biases.
      """
      num_layers = len(self.model_weights)
      num_cols = num_layers
      if show_biases:
        num_rows = 2

        fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 3))
        axes = axes.flatten()

        for layer in range(num_layers):
          ax = axes[layer]
          self.visualize_layer_heatmap(layer, level='Weights', cmap="bwr", ax=ax)
          ax.axis('off')

        for layer in range(num_layers):
          ax = axes[num_layers+layer]
          self.visualize_layer_heatmap(layer, level='Biases', cmap="bwr", ax=ax)
          ax.axis('off')

      else:
        num_rows = 1

        fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))
        axes = axes.flatten()

        for layer in range(num_layers):
          ax = axes[layer]
          self.visualize_layer_heatmap(layer, level='Weights', cmap="bwr", ax=ax)
          ax.axis('off')

      plt.tight_layout(rect=[0, 0, 1, 1])
      plt.show()

    # ----------------------------------------------------
    # Εκμεταλλευόμενοι το γεγονός ότι τα losses αποθηκεύονται στη λίστα model_losses της init, μπορούμε να δώσουμε και συνάρτηση που παράγει το σχετικό plot:

    def plot_training_loss(self):
      if not self.model_losses:
          print("Η λίστα model_losses είναι άδεια")
          return

      plt.figure(figsize=(8, 4))
      plt.plot(np.arange(len(self.model_losses)), self.model_losses, marker='o', linestyle='-')
      plt.title("Εξέλιξη του Training Loss:")
      plt.xlabel("Checkpoint (show_step)")
      plt.ylabel("Cross-Entropy Loss")
      plt.grid(True)
      plt.tight_layout()
      plt.show()